---
title: "How to Improve Data Validation in Five Steps<br><br>"
subtitle: ""
author: "Danilo Freire<br>"
date: '<font size="6">20^th^ of January, 2021</font><br><br> `r icon::fa("link")` <a href="http://danilofreire.github.io" target="_blank"><font size="5" color="white">danilofreire.github.io</font></a><br> `r icon::fa("github")` <a href="http://github.com/danilofreire/mercatus-analytics-papers" target="_blank"><font size="5" color="white">github.com/danilofreire/mercatus-analytics-papers</font></a><br>'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      countIncrementalSlides: false
      highlightLines: true
      highlightStyle: github
      ratio: 16:10
      titleSlideClass: [middle, left]
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
options(htmltools.dir.version = FALSE)
list.of.packages <- c("xaringan", "xaringanthemer")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(xaringanthemer)
style_mono_accent(
  base_color = "#1c5253",
  header_h1_font_size = "2.25rem",
  header_h2_font_size = "1.85rem",
  header_font_google  = google_font("Josefin Sans"),
  text_font_google    = google_font("Montserrat", "300", "300i"),
  text_font_size      = "1.45rem",
  code_font_google    = google_font("Fira Mono")
)
```

# Motivation

* Social scientists produce and consume more data than ever before

* Governments have made public data available to users to download directly or
  via APIs

* Sparked a large interest in tools to manage and analyse data

* Standardised methods to reproduce computational analysis, store scripts and
  data, run statistical models

---

# Motivation

* However, we have little guidance on how to validate new datasets

* Not covered in undergraduate classes, _ad hoc_ advice in graduate courses

* Most the literature comes from computer science, deals with specific types
  of data (high-frequency, very fine scale)

* Little discussion on conceptual validity or measurement errors

--

* _What rules should social scientists follow if they want to start collecting
  new data?_

---

# What I do

* Give some practical advice on how to start a data validation process

* Provide a short reading that scholars can use to introduce the topic to
  undergraduate students

* Offer additional readings to those interested in theoretical discussions about data
  validation

* Offer a simple check-list that experienced users may adopt to quickly assess
  the quality of their new data

* Five steps: technical consistency, logical consistency, content validity,
  data generation validity, and convergent validity

---

# Step 1: Technical consistency

* Easiest part of data validation...


--

* Very often overlooked even by experienced users


--

* **Technical consistency**: _data should be machine-readable and as intuitive as possible to humans_


--

* Others should be able to analyse your data immediately after receiving it

* What are the best practices in computer science?

---

# Tidy data

* [Tidy data](https://vita.had.co.nz/papers/tidy-data.html)
  - Each column represents one variable
  - Each row represents one observation
  - Each observational unit forms a table

![Tidy data](../data-validation-paper/tidy.png)

---

# Column types

* `ID` column clearly identified

* Strings as character vectors; numbers as numeric values (floating points or integers); binary indicators as boolean vectors (`TRUE` or `FALSE`); and categorical variables as factors (ESSnet ValiDat Foundation 2018)

* UTF-8 encoding (96% of all internet websites)

* `YYYY-MM-DD` (ISO 8601 standard)

* Files in `.csv` whenever possible

---

# Git and GitHub

.pull-left[
<img src="final.gif" width="500" height="550" class="center"/>
]

--

.pull-right[
* _Please_ use version control software

* [Git](https://git-scm.com) and [GitHub](http://github.com) are great tools

* Version control your data, scripts, and documentation
]

---

# Step 2: Logical consistency

* **Logical consistency**: _the elaboration of logical validation rules to assess the consistency of recorded values_

* Requires _a priori_ knowledge; logical validation methods are domain-specific by design

* However, we can use boolean statements to evaluate the data (`TRUE` or `FALSE`)

* Example: a researcher measures the average population age and GDP per capita. Thus, the data should have no negative values or include zero

* IF $\texttt{age} \leq 0$ OR $\texttt{gdp_per_capita} \leq 0 \hspace{0.4cm} \texttt{== FALSE}$
* $\texttt{ELSE == TRUE}$

---

# Logical consistency

* What to do when no information on true values exist?

--

* Estimate upper and lower bounds for the variable using Monte Carlo

--

* First step: create a value set $S$ that seems plausible within your data

--

* Second step: add disturbances to $S$ by including coding errors, missing
  values, etc. This yield a variable $S + \epsilon = S'$

--

* Third step: apply the consistency test to $S'$

--

* Fourth step: repeat the process with other error rates to produce the
  distribution $S_n$

---

# Why is this useful?

* Not only applied to new datasets, but also useful to evaluate existing data
  - Compare similar datasets that measure the same latent variable

* Logical rules can be defined in advance

* Reduce false positives

* Included in preregistration plans and journal submissions

---

# Step 3: Content validity

* **Content validity**: _whether the variables correspond to the theoretical concepts they intend to measure_

* Most difficult task in the data validation process

* [John Gerring](https://www.jstor.org/stable/3235246)'s criteria:
  - Resonance, domain, differentiation, fecundity, consistency, and causal utility

---

# Resonance and domain

* **Resonance**: the variable name brings to mind the core idea of a concept 

* Mnemonic devices: readers have an intuitive understanding of the concept,
  even if the variable itself is difficult to operationalise
  - "Civic culture", "social capital"

* **Domain**: whether relevant parties agree with the concept being measured

* How _area specialists_ understand the measure

* Specialists and the lay community may have very different ideas of what
  "democracy" means

* Ideally, the more domains a concept covers, the better; if not possible,
  stick to those most widely accepted in the field

---

# Differentiation and fecundity

* **Differentiation**: the variable should include the unique aspects of a given concept

* Establish boundaries that separate your variable from similar ones
  - States are not the same as tribes or empires: what makes them different?

* **Fecundity**: parsimony

* It is important to say what the variable is, but it is equally important to
  say _what it is not_
  - Counterfactual thinking: what are the competing definitions for the
    concept?

---

# Consistency and causal utility

* **Consistency**: whether a concept retains its validity across different settings

* Focus on sufficient attributes:
  - A measure of democracy should explain not only current Western regimes

* **Causal utility**: researchers are able to test hypotheses in which the concept described is either the main cause or the expected effect

* In most cases, concepts designed to be employed as independent variables require fewer attributes to be causally useful than those created to be dependent variables

---

# Step 4: Data generation validity

* **Data generation validity**: _the relationship between the concepts the researcher wants to translate into numeric values and the data generating process_

* Coding might be biased or unreliable

* Two common problems:
  - Low intercoder reliability
  - Data aggregation challenges

---

# Low intercoder reliability

* Calculate intercoder agreement using two popular statistics, Cohenâ€™s Kappa and Krippendorff's Alpha (irr package for R)

* Report the results of these tests in the data documentation files; justify
  which level is acceptable

* What to do when intercoder reliability is low?

* Allow coders to discuss and explain to each other where they disagree

* If time allows, implement multiple coding rounds and modify the coding frame accordingly

* Apply item-response theory (IRT) models to convert ordinal data to latent variables

---

# Data aggregation issues

* Creating indexes always leads to some type of information loss

* First clarify which of the attributes will be combined and for what reason

* Theory comes in: aggregation rules should always be theory-driven
  - Why additivity?
  - Why not using different weights to attributes?

* If the theory is silent about the problem at hand, statistical solutions can
  help
  - Bayesian Factor Analysis to model latent traits

---

# Step 5: Convergent validity

* **Convergent validity**: how well the variables compare with well-documented cases

* Similar to logical consistency, but addresses external validity of the
  dataset

* Suppose that a previous case study in the same location states with high confidence that 20% of males have earnings lower than $2,000 per month. We can formulate a validation rule set as follows:

* IF $\texttt{gender == "male"}$
* THEN $\frac{count(\texttt{salary} \hspace{0.1cm} > \hspace{0.1cm} 2,000)}{count(\texttt{salary})} = 0.2$

---

# Selecting case studies to assess convergent validity

* Again, I borrow some ideas from [Gerring and Seawright](https://doi.org/10.1177%2F1065912907313077): seven case study selection strategies

* *Typical cases*, ones that best represent the intended relationship (low
  residuals)

* *Diverse cases*, to obtain a range (spread of the distribution)

* *Extreme cases*, to measure extreme values

* *Deviant cases*, to model unexpected values in cross-case comparisons

---

# Selecting case studies to assess convergent validity

* *Influential cases*, not representative, but have a particular effect

* *most similar*, cases similar to each other (matching)

* *most different*, no common factors except the dependent variable

---

# Conclusion

* "80% of data analysis is spent on the process of cleaning and preparing the
  data"...

* ...yet we give little training to our students on how to validate data

* My goal in this short paper was not to provide an abstract view of the data validation process, but to offer a few pieces of practical advice that social scientists may find useful

* I divided the data validation process in five steps, from technical consistency to convergent validity, and added reading suggestions for authors who would like to read more about the topics discussed here

---

# Conclusion

* Scholars should pay special attention to issues of logical consistency and content validity, which are particularly difficult parts of the data validation process

* Translating concepts into numeric values is more art than science

* But we can do a bit better to teach the craft to young scholars 

---

class: center, middle, inverse

# Thank you very much!

---

# Let's keep in touch!

<br><br><br>

* Danilo Freire:

  - [danilofreire@gmail.com](mailto:danilofreire@gmail.com)
  - <http://danilofreire.github.io>
  - <http://github.com/danilofreire/mercatus-analytics-papers>
