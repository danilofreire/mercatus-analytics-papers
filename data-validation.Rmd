---
title: How to Improve Data Validation in Five Steps
author: Danilo Freire[^info]
date: 6th of October, 2020
abstract: ""
fontsize: 12pt
bibliography: references.bib
biblio-style: apalike
output: pdf_document
header-includes:
   - \usepackage{libertine}
   - \usepackage{inconsolata}
   - \usepackage{setspace}
   - \usepackage{amsmath}
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# If you need to install any package while knitting the document
r <- getOption("repos")
r["CRAN"] <- "https://cran.rstudio.com/"
options(repos = r)
if (!require("kableExtra")) {
    install.packages("kableExtra")
}
```

[^info]: Independent researcher,
[danilofreire@gmail.com](mailto:danilofreire@gmail.com),
<https://danilofreire.github.io>.

# Introduction

Social scientists are awash with data. According to a recent estimate,
humans produce about 2.5 billion gigabytes of information every day, and 90
percent of the global data were created in the last two years alone
[@ibm2016trends]. Governments also generate more data than ever before, with a
growing number of state agencies opening their data and allowing users to
access public records directly or via APIs [e.g., @al2017regdata;
@mcdonnell2019congressbr].

In this context, researchers have developed a series of tools to obtain, clean,
and store data files. R and Python, two open source programming languages, have
become the _de facto_ standards for downloading and manipulating data
[@magoulas2020keyareas; @perkel2018jupyter]. Reproducible scripts are now a
common feature in academic studies, allowing researchers to easily share and
verify their analyses [@hoffler2017replication; @key2016we]. Scholars can also
store their data and code in public repositories, such as
[GitHub](http://github.com) or the [Harvard
Dataverse](https://dataverse.harvard.edu), which guarantee that academic materials
will be preserved for future reference [@king2007introduction].

Despite the significant progress in data analytics, data validation techniques
have received little attention in academia. We define data validation as "[...]
_an activity verifying whether or not a combination of values is a member of a
set of acceptable combinations_" [@ess2018methodology, 8]. One reason for this
omission is that data quality procedures are not as standardised as other
statistical methods, so users often need to create _ad hoc_ semantic rules to
compile new data [@mcmann2016strategies]. Moreover, scholars should engage with
several sources of information to establish conceptual validity, which requires
unique ways to aggregate abstract concepts into a set of plausible numeric
values [@munck2002conceptualizing; @schedler2012judgment].

In this paper, I present five steps to help scholars improve their data
validation process. My idea is to offer a short check-list that is useful to
both data developers and reviewers, so that all parties involved in the
validation procedures have a common understanding of what constitutes good
practices in the field. My recommendations are based on the standards set by
Eurostat, an agency that provides statistical information to the European Union
(EU), and on the recent work by @mcmann2016strategies and
@schedler2012judgment. I discuss how to create testable validation functions,
how to increase construct validity, and how to incorporate qualitative
knowledge to statistical measurements. I present the concepts according to
their level of abstraction, with the last three demanding more theorisation
from the authors.

# Five Steps Towards Better Data Validation Processes[^summary]

[^summary]: Due to space constraints, this section includes only a brief summary
of the five validation steps I will present in the paper. Each of the topics
will be expanded in the final article.

## Step 1: Technical Consistency 

Ensuring technical consistency is a straightforward part of the data validation
process, yet it is sometimes overlooked even by experienced scholars. Technical
consistency means that the data should be machine-readable and as intuitive as
possible to humans. The computer science literature has some important
suggestions in this regard. First, data should be "tidy", that is, in a format
where each column represents one variable, each row represents one observation,
and each observational unit forms a table [@wickham2014tidy, 4]. It is
important to add an identifying variable (primary key) that is unique across
all records [@van2019data, 1]. Also, variables should include only one
information and have appropriate column types. For instance, users should store
strings as character vectors, numbers as numeric values (floating points or
integers), binary indicators as boolean vectors (true or false), and
categorical variables as factors [@ess2018methodology, 10]. Whenever possible,
save tabular data in pure text format as any software can read it.
Comma-separated values files (`.csv`) have wide support across many operating
systems and computer languages.

## Step 2: Logical Consistency

The second step involves the elaboration of domain-specific validation rules.
In contrast with technical consistency, logical integrity requires rules that
incorporate _a priori_ knowledge regarding the data at hand, and as such there
are no rules of thumb available for this step of the data validation process.
However, we can apply a general framework to logical consistency. One can
create _validation functions_ to test the quality of the data, such as
conditional statements which produce a set of `TRUE` or `FALSE` responses
[@van2019data, 3]. 

For instance, if a researcher measures the average population age and GDP per
capita, the data should have no negative values or include zero. An `if`
statement can verify whether the observations conform to that rule: 

* IF `age` $\leq 0$ OR `gdp_per_capita` $\leq 0$ `== FALSE`
* ELSE `== TRUE`

Scholars can use similar validations functions to assess the quality of any
variable. If no prior information on the true values exist, users can establish
lower and upper bounds for the variable they describe [@ess2018methodology,
67--73].

## Step 3: Content Validity

Content validity assessment evaluates whether the variables correspond to the
abstract theoretical concept it intends to measure. This is the hardest
validation check and social scientists have written extensively about the
topic. Here I follow @gerring2001social and suggest that researchers check if
their variables meet six criteria: resonance, domain, differentiation,
fecundity, consistency, and causal utility. Resonance means that the variable
reflects an appropriate understanding of the concept. Domain considers when
relevant parties agree with the concept being measured. Differentiation
indicates that the variable should exclude information that do not conform to
the concept of interest. Similarly, fecundity indicates that the variable is
parsimonious and include only the necessary information relevant to each
concept. Consistency is an attribute that signals whether a concept retain its
validity across different settings. Finally, causal utility refers to the idea
that the measure is useful to particular areas of study, which the authors
should enumerate when constructing their data.

## Step 4: Data Generation Validity

This steps corresponds to the relationship between the concepts the researcher
wants to translate into quantifiable information and the data generating
process. More specifically, scholars have to be aware that their data
generating process may be biased or unreliable [@mcmann2016strategies, 12].
With regards to bias, scholars should address whether coders may have
inadvertently introduced their own views into the data collection stage.
If the data generating process is original, it is recommended to verify
intercoder agreement by reporting Cohen's Kappa or Krippendorff's Alpha
[@lombard2002content]. When the data come from
secondary sources and the scholar wants to combine them into an index, it is
good practice to describe how aggregation rules may change the results. One
suggestion is to use Bayesian factor analysis to merge the low-level
indicators, as Bayesian inference adds uncertainty to the resulting index
[@mcmann2016strategies, 15]. 

## Step 5: Convergent Validity

The last step concerns how well the variables compare with well-documented cases.
@mcmann2016strategies suggest that researchers should evaluate their dataset
against other data sources that cover the same topics, including qualitative
studies. We can borrow the idea of validation functions discussed above and apply
the same logic here. However, the statements come from domain knowledge, thus
authors and reviewers should be familiar with the specialised literature in
order to check the consistency of the convergence rules.  

Consider a data set with two variables: `gender` and `salary`. Suppose that a
previous case study in the same location states with high confidence that 20%
of males have earnings lower than $2,000 per month. We can formulate a
validation rule set as follows:

* IF `gender == "male"`
* THEN $\frac{count(\texttt{salary} \hspace{0.1cm} > \hspace{0.1cm} 2,000)}{count(\texttt{salary})} = 0.2$

The example shows how researchers can integrate _a priori_ information to their
estimates. Authors can select the case studies they want to analyse using
different criteria, such as to maximise variation, to evaluate a typical case,
to understand outliers, and so on. I refer the readers to @seawright2008case
for a comprehensive review of case selection methodologies.

Lastly, authors should explain the reasons behind eventual divergences between
their measurements and current theoretical expectations. Validation functions
have the advantage of being transparent to referees and editors, thus making
the assumptions explicit and allowing reviewers to assess the work using
logical statements. That said, it is recommended that authors investigate why
outliers occur and test whether coder characteristics or measurement
construction explain these differences. @mcmann2016strategies [p. 27-36]
provide an empirical case study on how to assess convergent validity.

# References
