---
title: How to Improve Data Validation in Five Steps
author: Danilo Freire[^info]
date: \today
abstract: ""
fontsize: 12pt
bibliography: ../automl-paper/references.bib
biblio-style: apalike
output:
   pdf_document:
     number_sections: true
header-includes:
   - \usepackage[UKenglish]{babel}
   - \usepackage[UKenglish]{isodate}
   - \usepackage{libertine}
   - \usepackage[libertine]{newtxmath}
   - \usepackage{inconsolata}
   - \usepackage{setspace}
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# If you need to install any package while knitting the document
r <- getOption("repos")
r["CRAN"] <- "https://cran.rstudio.com/"
options(repos = r)
if (!require("kableExtra")) {
    install.packages("kableExtra")
}
```

[^info]: Independent researcher,
[danilofreire@gmail.com](mailto:danilofreire@gmail.com),
<https://danilofreire.github.io>.

\doublespacing

# Introduction

Social scientists are awash with data. According to a recent estimate, humans
produce about 2.5 billion gigabytes of information every day, and 90 percent of
the global data were created in only two years [@ibm2016trends]. Governments
also generate more data than ever before, with a growing number of agencies
opening their archives and allowing users to access public records directly or
via APIs [e.g., @al2017regdata; @mcdonnell2019congressbr].

In this context, researchers have developed a series of tools to obtain, clean,
and store data files. R and Python, two open source programming languages, have
become the _de facto_ standards for downloading and manipulating data
[@magoulas2020keyareas; @perkel2018jupyter]. Reproducible scripts are now a
common feature in academic studies, allowing researchers to easily share and
verify their analyses [@hoffler2017replication; @key2016we]. Scholars can also
store their data and code in public repositories, such as
[GitHub](http://github.com) or the [Harvard
Dataverse](https://dataverse.harvard.edu), which guarantee that academic
materials will be preserved for future reference [@king2007introduction].

Although there has been significant progress in data analytics, data validation
techniques have received little attention in academia. Data validation is
defined as "[...] _an activity verifying whether or not a combination of values
is a member of a set of acceptable combinations_" [@ess2018methodology, 8]. One
reason for this omission is that data quality procedures are not as
standardised as other statistical methods, so users often need to create _ad
hoc_ semantic rules to compile new data [@mcmann2016strategies]. Moreover,
scholars should engage with several sources of information to establish
conceptual validity, which requires unique ways to aggregate abstract concepts
into a set of plausible numeric values [@munck2002conceptualizing;
@schedler2012judgment].

In this paper, I present five steps to help scholars improve their data
validation process. My idea is to offer a short check-list that is useful to
both data developers and reviewers, so all parties involved in the validation
procedures have a common understanding of what constitutes good practices in
the field. My recommendations are based on the standards set by Eurostat, an
agency that provides statistical information to the European Union (EU), and on
recent work by @mcmann2016strategies and @schedler2012judgment. I discuss how
to create testable validation functions, how to increase construct validity,
and how to incorporate qualitative knowledge to statistical measurements. I
present the concepts according to their level of abstraction, with the last
three demanding more theorisation than the first two, and I provide practical
examples on how scholars can add the suggestions below to their work.

# Five Steps Towards Better Data Validation Processes

## Step 1: Technical Consistency 

Ensuring technical consistency is perhaps the easiest task in the data
validation process, yet it is often overlooked even by experienced scholars.
Technical consistency means that the data should be machine-readable and as
intuitive as possible to humans. More specifically, scholars have to ensure
that their data do not produce parsing errors, that values corresponding to
variables and observations are clearly identified, and that other researchers
can analyse the data as soon as they receive them.

The computer science literature has some important suggestions in this regard.
First, data should be "tidy", that is, in a format where each column represents
one variable, each row represents one observation, and each observational unit
forms a table [@wickham2014tidy, 4]. 

It is important to add an identifying
variable (primary key) that is unique across all records [@van2019data, 1].
Also, variables should include only one information and have appropriate column
types. For instance, users should store strings as character vectors, numbers
as numeric values (floating points or integers), binary indicators as boolean
vectors (true or false), and categorical variables as factors
[@ess2018methodology, 10]. Whenever possible, save tabular data in pure text
format as any software can read it. Comma-separated values files (`.csv`) have
wide support across many operating systems and computer languages.

## Step 2: Logical Consistency

The second step involves the elaboration of domain-specific validation rules.
In contrast with technical consistency, logical integrity requires rules that
incorporate _a priori_ knowledge regarding the data at hand, and as such there
are no rules of thumb available for this step of the data validation process.
However, we can apply a general framework to logical consistency. One can
create _validation functions_ to test the quality of the data, such as
conditional statements which produce a set of `TRUE` or `FALSE` responses
[@van2019data, 3]. 

For instance, if a researcher measures the average population age and GDP per
capita, the data should have no negative values or include zero. An `if`
statement can verify whether the observations conform to that rule: 

* IF `age` $\leq 0$ OR `gdp_per_capita` $\leq 0$ `== FALSE`
* ELSE `== TRUE`

Scholars can use similar validations functions to assess the quality of any
variable. If no prior information on the true values exist, users can establish
lower and upper bounds for the variable they describe [@ess2018methodology,
67--73].

## Step 3: Content Validity

Content validity assessment evaluates whether the variables correspond to the
abstract theoretical concept it intends to measure. This is the hardest
validation check and social scientists have written extensively about the
topic. Here I follow @gerring2001social and suggest that researchers check if
their variables meet six criteria: resonance, domain, differentiation,
fecundity, consistency, and causal utility. Resonance means that the variable
reflects an appropriate understanding of the concept. Domain considers when
relevant parties agree with the concept being measured. Differentiation
indicates that the variable should exclude information that do not conform to
the concept of interest. Similarly, fecundity indicates that the variable is
parsimonious and include only the necessary information relevant to each
concept. Consistency is an attribute that signals whether a concept retain its
validity across different settings. Finally, causal utility refers to the idea
that the measure is useful to particular areas of study, which the authors
should enumerate when constructing their data.

## Step 4: Data Generation Validity

This steps corresponds to the relationship between the concepts the researcher
wants to translate into quantifiable information and the data generating
process. More specifically, scholars have to be aware that their data
generating process may be biased or unreliable [@mcmann2016strategies, 12].
With regards to bias, scholars should address whether coders may have
inadvertently introduced their own views into the data collection stage.
If the data generating process is original, it is recommended to verify
intercoder agreement by reporting Cohen's Kappa or Krippendorff's Alpha
[@lombard2002content]. When the data come from
secondary sources and the scholar wants to combine them into an index, it is
good practice to describe how aggregation rules may change the results. One
suggestion is to use Bayesian factor analysis to merge the low-level
indicators, as Bayesian inference adds uncertainty to the resulting index
[@mcmann2016strategies, 15]. 

## Step 5: Convergent Validity

The last step concerns how well the variables compare with well-documented cases.
@mcmann2016strategies suggest that researchers should evaluate their dataset
against other data sources that cover the same topics, including qualitative
studies. We can borrow the idea of validation functions discussed above and apply
the same logic here. However, the statements come from domain knowledge, thus
authors and reviewers should be familiar with the specialised literature in
order to check the consistency of the convergence rules.  

Consider a data set with two variables: `gender` and `salary`. Suppose that a
previous case study in the same location states with high confidence that 20%
of males have earnings lower than $2,000 per month. We can formulate a
validation rule set as follows:

* IF `gender == "male"`
* THEN $\frac{count(\texttt{salary} \hspace{0.1cm} > \hspace{0.1cm} 2,000)}{count(\texttt{salary})} = 0.2$

The example shows how researchers can integrate _a priori_ information to their
estimates. Authors can select the case studies they want to analyse using
different criteria, such as to maximise variation, to evaluate a typical case,
to understand outliers, and so on. I refer the readers to @seawright2008case
for a comprehensive review of case selection methodologies.

Lastly, authors should explain the reasons behind eventual divergences between
their measurements and current theoretical expectations. Validation functions
have the advantage of being transparent to referees and editors, thus making
the assumptions explicit and allowing reviewers to assess the work using
logical statements. That said, it is recommended that authors investigate why
outliers occur and test whether coder characteristics or measurement
construction explain these differences. @mcmann2016strategies [p. 27-36]
provide an empirical case study on how to assess convergent validity.

\newpage

# References
