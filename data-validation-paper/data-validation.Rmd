---
title: How to Improve Data Validation in Five Steps
author: Danilo Freire[^info]
date: \today
abstract: ""
fontsize: 12pt
bibliography: ../automl-paper/references.bib
biblio-style: apalike
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    number_sections: yes
    keep_tex: no
    template: ../automl-paper/article-template.latex
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# If you need to install any package while knitting the document
r <- getOption("repos")
r["CRAN"] <- "https://cran.rstudio.com/"
options(repos = r)
```

[^info]: Independent researcher,
[danilofreire@gmail.com](mailto:danilofreire@gmail.com),
<https://danilofreire.github.io>.

\doublespacing

# Introduction

Social scientists are awash with data. According to a recent estimate, humans
produce about 2.5 billion gigabytes of information every day, and 90 percent of
the global data were created in only two years [@ibm2016trends]. Governments
also generate more data than ever before, with a growing number of agencies
opening their archives and allowing users to access public records directly or
via APIs [e.g., @al2017regdata; @mcdonnell2019congressbr].

In this context, researchers have developed a series of tools to obtain, clean,
and store data files. R and Python, two open source programming languages, have
become the _de facto_ standards for downloading and manipulating data
[@magoulas2020keyareas; @perkel2018jupyter]. Reproducible scripts are now a
common feature in academic studies, so researchers can easily share and verify
their analyses [@hoffler2017replication; @key2016we]. Scholars can also store
data and code in public repositories, such as [GitHub](http://github.com) or
the [Harvard Dataverse](https://dataverse.harvard.edu), which guarantee that
academic materials will be preserved for future reference
[@king2007introduction].

Although there has been significant progress in data analytics, data validation
techniques have received little attention in academia. Data validation is
defined as "[...] _an activity verifying whether or not a combination of values
is a member of a set of acceptable combinations_" [@ess2018methodology, 8]. One
reason for this omission is that data quality procedures are not as
standardised as other statistical methods, so users often need to create _ad
hoc_ semantic rules to compile new data [@mcmann2016strategies]. Moreover,
scholars have to engage with several sources of information to establish
conceptual validity and translate abstract concepts into plausible numeric
values [@munck2002conceptualizing; @schedler2012judgment].

In this paper, I present five steps to help scholars improve their data
validation process. My idea is to offer a short check-list that is useful to
both data developers and reviewers, so all parties involved in the validation
procedures have a common understanding of what constitutes good practices in
the field. My suggestions are based on the standards set by Eurostat, an
agency that provides statistical information to the European Union (EU), and on
recommendations by @gerring2001social, @mcmann2016strategies, and
@schedler2012judgment. I discuss how to create testable validation functions,
how to increase construct validity, and how to incorporate qualitative
knowledge to statistical measurements. I present the concepts according to
their level of abstraction, with the last three demanding more theorisation
than the first two, and I provide practical examples on how scholars can add
the suggestions below to their work.

# Five Steps Towards Better Data Validation Processes

## Step 1: Technical Consistency 

Ensuring technical consistency is perhaps the easiest task in the data
validation process, yet it is often overlooked even by experienced scholars.
Technical consistency means that the data should be machine-readable and as
intuitive as possible to humans. More specifically, scholars have to ensure
that their data do not produce parsing errors, that values corresponding to
variables and observations are clearly identified, and that other researchers
can analyse the data as soon as they receive them.

The computer science literature has some important suggestions in this regard.
First, data should be "tidy", that is, in a format where each column represents
one variable, each row represents one observation, and each observational unit
forms a table [@wickham2014tidy, 4]. Although the suggestions seem simple,
scholars sometimes break these rules when building a new data. It is not
uncommon to see multiple variables stored as a single column (e.g., race and
gender together) or one variable divided into many columns (e.g., one column
per level of income). Fortunately, most datasets can be tidied up with simple
operations, such as pivoting tables, splitting or combining columns, or
filtering values. Please refer to @wickham2016r4ds for more information on how
to clean messy data. 

```{r, cache=TRUE, echo=FALSE, fig.cap="A tidy dataset. Source: Wickham and Grolemund (2016).", out.width = "80%", fig.align="center"}
knitr::include_graphics("~/Documents/github/mercatus-analytics-papers/data-validation-paper/tidy.png")
```

It is important to add an identifying variable (primary key) that is unique
across all records [@van2019data, 1]. This is particularly relevant when
scholars need to merge different datasets, as the primary keys have to be the
same across all tables. Also, variables should have appropriate column types,
or the machine may not be able to parse the values correctly. For instance,
users should store strings as character vectors, numbers as numeric values
(floating points or integers), binary indicators as boolean vectors (`TRUE` or
`FALSE`), and categorical variables as factors [@ess2018methodology, 10].
Whenever possible, save tabular data in pure text format as any software can
read it. Comma-separated values files (`.csv`) have wide support across many
operating systems and can be easily opened in Excel, R, and Python with no
errors.

Variables containing textual data or times and dates need special attention as
they are prone to encoding issues. There are dozens of character encoding
standards, but a recent survey shows that Unicode Transformation Format --
8-bit (UTF-8) is used on 96% of all internet websites [@w3techs2020encoding].
In this sense, scholars should use UTF-8 to store text variables as it is
widely accessible. UTF-8 is able to convert characters from many alphabets and
its most recent version also stores emojis. Regarding times and dates, scholars
are advised to record time in the ISO 8601 standard ISO. The ISO standard uses
the Gregorian calendar and a 24-h timescale [@van2018statistical, 52]. Dates
are stored in the `YYYY-MM-DD` format, in which `YYYY` represents the year,
`MM` the month, and `DD` the day. Time of day should be expressed as
`hh:mm:ss`, for hour, minute, and second, respectively.

Although not strictly required, it is recommended that researchers use version
control to keep track of changes and ensure the technical consistency of their
data files. [Git](https://git-scm.com) is the most popular version control
system. While it was first designed to manage computer code, Git allows users
to record and recall any particular version of a given document, and it works
well with pure text files like `.csv`. This increases transparency in academic
research, as others can trace all steps of the data management process and
serve as a reliable back up in the case of data loss [@ram2013git]. Moreover,
contributors can modify the files and merge them to the main Git repository,
what facilitates collaboration and review at every stage of the research
project. Since users can quickly revert file changes with Git, the process is
risk-free. For more information about Git, please visit <https://git-scm.com>.

## Step 2: Logical Consistency

The second step involves the elaboration of logical validation rules. In
contrast with technical integrity, logical consistency requires rules that
incorporate _a priori_ knowledge from a particular scientific field, so logical
validation methods are domain-specific by design. However, we can apply a
general framework to logical consistency based on conditional expressions,
which follow simple evaluations of boolean statements. In other words, scholars
may use validation functions that produce a set of `TRUE` or `FALSE` responses
to assess the consistency of their datasets [@van2019data, 3]. 

For instance, if a researcher measures the average population age and GDP per
capita, the data should have no negative values or include zero. An `if`
statement can verify whether the observations conform to that rule: 

* IF `age` $\leq 0$ OR `gdp_per_capita` $\leq 0$ `== FALSE`
* ELSE `== TRUE`

Scholars can use similar validations functions to check the logical
validity of any variable for which prior information is available. Users can
also combine conditional statements and check the quality of related variables
with a single function. As an example, if a dataset contains information about
the location of the subjects, such as city and street name, one can assume that
the postal code is the same if the two values are identical [@van2019data, 12].
Translating the rule into conditional expressions, the code would be:

* IF `city`~`[i]`~ == `city`~`[j]`~ AND `street`~`[i]`~ == `street`~`[j]`~ 
* THEN `postal`~`[i]`~ $\equiv$ `postal`~`[j]`~

There are cases, however, for which no exact information on the true values
exist. But social scientists can still verify the consistency of their data
using conditional rules. Instead of focusing on a particular boolean outcome,
researchers should estimate upper and lower bounds for the variable they want
to check. One way to do so is via Monte Carlo simulations. The
@ess2018methodology [72] proposes an easy yet effective method to estimate
logical consistency bounds for a given variable. First, create a set of values
$S$ that seems plausible according to the related literature. Second, add
disturbances to the dataset $S$ by simulating cases with measurement error,
missing values, mistakes in the data entry process, or other statistical issues
that are common to that type of data. This yields a variable $S'$. Then, apply
the consistency test to $S'$. In the final step, repeat this process several
times and change the number of wrong observations to create a distribution of
rule statistics from the simulated $S'$~_n_~ variables. This method produces
the lower and upper bounds required for the data, assuming that $S$
correctly approximates the true values of the chosen variable.

While in the section I discuss how rules can be applied to new datasets, they
may also provide interesting insights when used to evaluate existing data. One
possible avenue for research is to compare whether certain conditional
statements produce different outcomes in datasets that measure the same
phenomena, such as level of democracy or changes in political regimes.
Observations that do not conform to the rules should be contrasted and
explained, and if that is case, imputed to credible values. Indeed,
@dewaal2017imputation suggests that imputation methods that both preserve
statistical properties of the variables and conform to rule restrictions are
the best way to fill missing data, even if so far they are difficult to
estimate.

Testing data with conditional expressions provide another benefit for
researchers. Since logical rules can be defined in advance, they may be
included in a preregistration plan. Registering the study design before data
collection or analysis reduces the chances of "data dredging", cases in which
researchers release only the analyses that support their hypotheses
[@klein2018practical]. Preregistration increases the credibility of the
findings and may be even submitted directly for publication in the form of
registered reports [@chambers2013registered]. Journals can evaluate the
research design before scholars know the results, and the manuscript's
acceptance is independent of the final data. As of late 2020, 275 journals
use registered reports as a regular or one-off submission option [@cos2020rr].

## Step 3: Content Validity

Content validity refers to whether the variables correspond to the theoretical
concepts they intend to measure. This is a difficult task as there are no
hard-and-fast rules on how to map concepts into values. Here I follow Gerring
[-@gerring1999makes; -@gerring2001social; -@gerring2011social] and suggest that
researchers should check if their variables meet six criteria: resonance,
domain, differentiation, fecundity, consistency, and causal utility. I explain
each of them in further detail below.

*Resonance* means that the variable name brings to mind the core idea of a
concept. A clear definition is one that includes a simple word that is used in
common language, and that quickly conveys the point the authors are trying to
make. Terms that resonate are akin to mnemonic devices, something that helps
readers to remember what the variable means long after they see it
[@gerring1999makes, 370]. Concepts like "social capital" or "civic culture"
might be difficult to measure, but they do invoke an intuitive understanding of
the concepts authors refer to [@bjornskov2013social].

*Domain* considers whether relevant parties agree with the concept being
measured. The idea of domain is similar to that of resonance, but it describes
how particular audiences, mainly area specialists, interpret the concept one
intends to describe [@mcmann2016strategies, 9]. For example, the domain of
"democracy" as measured by political indices may differ substantially from what
the lay community generally understands as the "government of the people"
[@munck2002conceptualizing]. In that respect, the concept should embrace as
many domains as possible, although it should strive first for internal validity
and consistency [@tortola2017clarifying, 241]. Thus, it is essential that the
researcher has a firm idea of what his or her target public expects from the
concept.

*Differentiation* indicates that the variable should include the unique aspects
of a given concept. In other words, it entails that one should find what makes
a concept distinct from related terms, and the sharper those boundaries are,
the stronger the validity of the concept. As @gerring1999makes [375] notes, any
good definition of "state" has to single out what characteristics are
particular to states and do not appear in other forms of social organisation,
such as tribes or empires. Many concepts in the social sciences cause confusion
precisely because they often include traits that not exclusive to these
categories. For instance, if one defines "armed conflict" as "the use of force
by the state or civilians against other groups", it is not possible to
differentiate such cases from episodes of lynchings or genocide.

Similarly, *fecundity* indicates that the variable is parsimonious and exclude
all information that is not related to the concept. It refines the
differentiation attribute and highlights that it is not only required for
scholars to affirm what the concept is, but also to show what *it is not*
[@gerring2001social, 92]. This exercise involves counterfactual thinking, and
it is not always clear which unrealised outcome scholars should focus on. One
suggestion may be to start with competing definitions and remove
characteristics that conflate the original concept with similar behaviours. For
instance, a corruption variable should exclude private benefits that do not
originate from someone's governmental position [@mcmann2016strategies, 10].

*Consistency* is an attribute that signals whether a concept retain its
validity across different settings [@mcmann2016strategies, 11]. An indicator of
liberal democracy should be able to explain not only Western regimes, but to
identify liberal traits in countries that do not share a similar background and
retain its consistency over time. In this sense, the variable should
preferably measure sufficient attributes of the concept, which can be easily
identified in other cases.

Lastly, *causal utility* means that researchers are able to test hypotheses in
which the concept described is either the main cause or the expected effect
[@gerring1999makes, 367]. In most cases, concepts designed to be employed as
independent variables require fewer attributes to be causally useful than those
created to be dependent variables [@gerring2011social, 130]. While it is hard
to ensure that a concept is completely exogenous from other theoretical
constructs, authors should avoid a definition that includes known confounders
connect the concept to background factors [@gerring2011social, 130].

## Step 4: Data Generation Validity

This step corresponds to the relationship between the concepts the researcher
wants to translate into numeric values and the data generating process. More
specifically, scholars need to be aware that their data generating process may
be biased or unreliable [@mcmann2016strategies, 12]. Problems may arise either
when gathering new data or using secondary sources. Here I focus on two common
issues, low intercoder reliability and data aggregation challenges.

With regards to novel datasets, scholars should address whether coders have
inadvertently introduced their own views during the data collection stage.
Although intercoder reliability does not guarantee that the data are correct,
disagreements between coders raise a red flag about the validity of the
recorded values [@kolbe1991content, 248]. A first step is to calculate
intercoder agreement using two popular statistics, Cohen's Kappa and
Krippendorff's Alpha [@lombard2002content]. While these tests have their
limitations, they are easy to calculate and are available in many statistical
languages. R users have the `irr` package [@gamer2019irr] that provides
functions to estimate those statistics for any dataset. Scholars are also
expected to report the results of these tests in their materials, as well as a
justification for the minimum acceptable level of intercoder reliability they
have adopted [@lombard2002content, 600].

There are a few recommendations on how to proceed when intercoder agreement is
low. First, provide training and clear guidelines to the raters. Allowing them
to discuss and explain to each other where they disagree can also bring
substantial increases to intercoder reliability [@o2020intercoder]. Second, if
time allows, implement multiple coding rounds and modify the coding frame
accordingly until intercoder agreement reaches a desired level
[@macphail2016process]. A final suggestion would be to apply item-response
theory (IRT) models to convert ordinal data to latent variables, as they allow
for intercoder variation in skill and in perceived scale differences
[@marquardt2018irt].

When the data come from secondary sources and the scholar wants to combine them
into an index, it is good practice to describe how aggregation rules may change
the results. Loss of information is inevitable when aggregating attributes into
an index, so scholars need to first clarify which of the attributes will be
combined and for what reason. This is where theory comes in, as aggregation
rules should be theoretically motivated. For instance, it is unclear whether
additivity, the default method for merging low-level attributes, is the best
aggregation rule for most indices, or why scholars do not weight their
attributes more often [@munck2002conceptualizing, 24]. As long as the moves are
theoretically consistent, researchers can use very different methods for index
construction.

There are cases, nevertheless, where one has no _a priori_ knowledge about what
features to include in an aggregate measure. It can either be because the
current literature offers little guidance on the topic or because features are
multicollinear. One suggestion is to use Bayesian factor analysis (BFA) as a
technique to model latent traits [@conti2014bayesian]. While researchers have
long used principal component analysis (PCA) to reduce the dimensionality of a
dataset, BFA has several advantages over PCA. BFA propagates uncertainty in the
estimates, allows for correlated factors and control variables, and can either
decide automatically or let the users include as many factors as they see fit
into the index. The R package `BayesFM` [@piatek2020bayesfm] performs the
analysis described here. 

## Step 5: Convergent Validity

The last step concerns how well the variables compare with well-documented cases.
@mcmann2016strategies suggest that researchers should evaluate their dataset
against other data sources that cover the same topics, including qualitative
studies. We can borrow the idea of validation functions discussed above and apply
the same logic here. However, the statements come from domain knowledge, thus
authors and reviewers should be familiar with the specialised literature in
order to check the consistency of the convergence rules.  

Consider a data set with two variables: `gender` and `salary`. Suppose that a
previous case study in the same location states with high confidence that 20%
of males have earnings lower than $2,000 per month. We can formulate a
validation rule set as follows:

* IF `gender == "male"`
* THEN $\frac{count(\texttt{salary} \hspace{0.1cm} > \hspace{0.1cm} 2,000)}{count(\texttt{salary})} = 0.2$

The example shows how researchers can integrate _a priori_ information to their
estimates. Authors can select the case studies they want to analyse using
different criteria. @seawright2008case offer an interesting comparison between
seven case selection methods and the corresponding large-_N_ statistical
reasoning behind the choices. The first method is selecting the _typical_ case,
one that best represents the intended relationship. This is the equivalent of
analysing low-residual cases, those which are very close to the fitted
statistical curve. Authors can choose _diverse_ cases to compare if they are
interested in obtaining a range for their variables, or points in an ordinal
scale. Third, _extreme_ cases describe observations that lie in the tails of
the distribution. _Deviant_ cases are akin to outliers in statistical
modelling. _Influential_ cases are those which are often not representative,
but have a particular effect caused by independent variables. The last two
cases are similar to those I have discussed above, _most similar_, which
parallels matching techniques in large-_N_ studies, and _most different_, its
opposite. Please refer to @seawright2008case for more information on case
selection strategies.

Finally, authors should explain the reasons behind eventual divergences between
their measurements and current theoretical expectations. Validation functions
have the advantage of being transparent to referees and editors, thus making
the assumptions explicit and allowing reviewers to assess the work using
logical statements. That said, it is recommended that authors investigate why
outliers occur and test whether coder characteristics or measurement
construction explain these differences. @mcmann2016strategies [p. 27-36]
provide an empirical case study on how to assess convergent validity.

# Conclusion

Data validation is a crucial yet undertheorised topic in the social sciences.
While estimation methods have made significant progress over the last decades,
data validation procedures remain largely absent from university courses and
academic textbooks. This is at odds with the famous saying that "80% of data
analysis is spent on the process of cleaning and preparing the data"
[@wickham2014tidy, 1] and with the growing number of datasets social scientists
have amassed recently. My goal in this short paper was not to provide an
abstract view of the data validation process, but to offer a few pieces of
practical advice that social scientists may find useful when creating a new
dataset or assessing the properties of existing ones. I divided the data
validation process in five steps, from technical consistency to convergent
validity, and added reading suggestions for authors who would like to read more
about the topics discussed here.

I encourage authors to pay special attention to issues of logical consistency
and content validity, which are particularly difficult parts of the data
validation process. Translating concepts into numeric values is more art than
science, even more so in areas where many foundational ideas remain contested.
Thus, careful theoretical considerations are key to better measurement. Another
important part of the data validation process is reproducibility. All steps of
the data collection process should be documented and shared along with the
final results. Reproducible research leads to timely feedback, quality reviews,
and stronger academic collaborations, so scholars have an incentive to adopt
reproducible methods in their work. Computer science and its many successful
open source projects provide good evidence in favour of greater research
transparency. As stated by Eric Raymond [-@raymond2001cathedral, 30], a
software developer, "given enough eyeballs, all bugs are shallow". Maybe the
same is true in our discipline.

\newpage

# References
