---
title: How to Improve Data Validation in Five Steps
author: Danilo Freire[^info]
date: \today
abstract: ""
fontsize: 12pt
bibliography: ../automl-paper/references.bib
biblio-style: apalike
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    number_sections: yes
    keep_tex: no
    template: ../automl-paper/article-template.latex
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# If you need to install any package while knitting the document
r <- getOption("repos")
r["CRAN"] <- "https://cran.rstudio.com/"
options(repos = r)
if (!require("kableExtra")) {
    install.packages("kableExtra")
}
```

[^info]: Independent researcher,
[danilofreire@gmail.com](mailto:danilofreire@gmail.com),
<https://danilofreire.github.io>.

\doublespacing

# Introduction

Social scientists are awash with data. According to a recent estimate, humans
produce about 2.5 billion gigabytes of information every day, and 90 percent of
the global data were created in only two years [@ibm2016trends]. Governments
also generate more data than ever before, with a growing number of agencies
opening their archives and allowing users to access public records directly or
via APIs [e.g., @al2017regdata; @mcdonnell2019congressbr].

In this context, researchers have developed a series of tools to obtain, clean,
and store data files. R and Python, two open source programming languages, have
become the _de facto_ standards for downloading and manipulating data
[@magoulas2020keyareas; @perkel2018jupyter]. Reproducible scripts are now a
common feature in academic studies, allowing researchers to easily share and
verify their analyses [@hoffler2017replication; @key2016we]. Scholars can also
store their materials in public repositories, such as
[GitHub](http://github.com) or the [Harvard
Dataverse](https://dataverse.harvard.edu), which guarantee that academic
materials will be preserved for future reference [@king2007introduction].

Although there has been significant progress in data analytics, data validation
techniques have received little attention in academia. Data validation is
defined as "[...] _an activity verifying whether or not a combination of values
is a member of a set of acceptable combinations_" [@ess2018methodology, 8]. One
reason for this omission is that data quality procedures are not as
standardised as other statistical methods, so users often need to create _ad
hoc_ semantic rules to compile new data [@mcmann2016strategies]. Moreover,
scholars should engage with several sources of information to establish
conceptual validity, which requires unique ways to aggregate abstract concepts
into a set of plausible numeric values [@munck2002conceptualizing;
@schedler2012judgment].

In this paper, I present five steps to help scholars improve their data
validation process. My idea is to offer a short check-list that is useful to
both data developers and reviewers, so all parties involved in the validation
procedures have a common understanding of what constitutes good practices in
the field. My recommendations are based on the standards set by Eurostat, an
agency that provides statistical information to the European Union (EU), and on
recent work by @mcmann2016strategies and @schedler2012judgment. I discuss how
to create testable validation functions, how to increase construct validity,
and how to incorporate qualitative knowledge to statistical measurements. I
present the concepts according to their level of abstraction, with the last
three demanding more theorisation than the first two, and I provide practical
examples on how scholars can add the suggestions below to their work.

# Five Steps Towards Better Data Validation Processes

## Step 1: Technical Consistency 

Ensuring technical consistency is perhaps the easiest task in the data
validation process, yet it is often overlooked even by experienced scholars.
Technical consistency means that the data should be machine-readable and as
intuitive as possible to humans. More specifically, scholars have to ensure
that their data do not produce parsing errors, that values corresponding to
variables and observations are clearly identified, and that other researchers
can analyse the data as soon as they receive them.

The computer science literature has some important suggestions in this regard.
First, data should be "tidy", that is, in a format where each column represents
one variable, each row represents one observation, and each observational unit
forms a table [@wickham2014tidy, 4]. Although the suggestions seem simple,
scholars sometimes break these rules when building a new data. It is not
uncommon to see multiple variables stored as a single column (e.g., race and
gender together) or one variable divided into many columns (e.g., one column
per level of income). Fortunately, most datasets can be tidied up with simple
operations, such as pivoting tables, splitting or combining columns, or
filtering values. Please refer to @wickham2016r4ds for more information on how
to clean messy data. 

```{r, echo=FALSE, fig.cap="A tidy dataset. Source: Wickham and Grolemund (2016).", out.width = "80%", fig.align="center"}
knitr::include_graphics("~/Documents/github/mercatus-analytics-papers/data-validation-paper/tidy.png")
```

It is important to add an identifying variable (primary key) that is unique
across all records [@van2019data, 1]. This is particularly relevant when
scholars need to merge different datasets, as the primary keys have to be the
same across all tables. Also, variables should have appropriate column types,
or the machine may not be able to parse the values correctly. For instance,
users should store strings as character vectors, numbers as numeric values
(floating points or integers), binary indicators as boolean vectors (`TRUE` or
`FALSE`), and categorical variables as factors [@ess2018methodology, 10].
Whenever possible, save tabular data in pure text format as any software can
read it. Comma-separated values files (`.csv`) have wide support across many
operating systems and can be easily opened in Excel, Python, and R with no
errors.

Variables containing textual data or times and dates need special attention as
they are prone to encoding issues. There are dozens of character encoding
standards, but a recent survey shows that Unicode Transformation Format --
8-bit (UTF-8) is used in about 96% of all internet websites
[@w3techs2020encoding]. In this sense, scholars should use UTF-8 to store text
variables as it is widely accessible. UTF-8 is able to convert characters from
many alphabets and its most recent version also stores emojis. Regarding times
and dates, users are advised to record time using the ISO 8601 standard ISO. It
uses the Gregorian calendar and a 24-h timescale [@van2018statistical, 52].
Dates are stored in the `YYYY-MM-DD` format, in which `YYYY` represents the
year, `MM` the month, and `DD` the day. The ISO standard suggests that time of
day should be expressed as `hh:mm:ss`, for hour, minute, and second,
respectively.

Although not strictly required, it is recommended that researchers use version
control to keep track of changes and ensure the technical consistency of their
data files. [Git](https://git-scm.com) is the most popular version control
system. While it was first designed to manage computer code, Git allows users
to record and recall any particular version of a given document, and it works
well with pure text files like `.csv`. This increases transparency in academic
research, as others can trace all steps of the data management process and
serve as a reliable back up in the case of data loss [@ram2013git]. Moreover,
contributors can modify the files and merge them to the main Git repository,
what facilitates collaboration and review at every stage of the research
project. Since users can quickly revert file changes with Git, the process is
risk-free. For more information about Git, please visit <https://git-scm.com>.

## Step 2: Logical Consistency

The second step involves the elaboration of domain-specific validation rules.
In contrast with technical integrity, logical consistency requires rules that
incorporate _a priori_ knowledge from a particular scientific field, so logical
validation methods are domain-specific by design. However, we can apply a
general framework to logical consistency based on conditional expressions,
which follow simple evaluations of boolean statements. In other words, scholars
may use validation functions that produce a set of `TRUE` or `FALSE` responses
to assess the consistency of their datasets [@van2019data, 3]. 

For instance, if a researcher measures the average population age and GDP per
capita, the data should have no negative values or include zero. An `if`
statement can verify whether the observations conform to that rule: 

* IF `age` $\leq 0$ OR `gdp_per_capita` $\leq 0$ `== FALSE`
* ELSE `== TRUE`

Scholars can use similar validations functions to assess the logical
consistency of any variable for which prior information is available. Users can
also combine conditional statements and check the quality of related variables
with a single function. As an example, if a dataset contains information about
the location of the subjects, such as city and street name, one can assume that
the postal code is the same if the two values are identical [@van2019data, 12].
In other terms:

* IF `city`~`[i]`~ == `city`~`[j]`~ AND `street`~`[i]`~ == `street`~`[j]`~ 
* THEN `postal`~`[i]`~ $\equiv$ `postal`~`[j]`~

There are cases, however, that no exact information on the true values exist.
But social scientists can still verify the consistency of their data using
conditional rules. Instead of focusing on a particular boolean outcome,
researchers should estimate upper and lower bounds for the variable they want
to check. One way to do so is via Monte Carlo simulations. The
@ess2018methodology [72] proposes a simple yet effective method to estimate
logical consistency bounds. First, create a set of values $S'$ that seems
plausible according to the related literature. Second, add disturbances to the
dataset $S'$ by simulating cases with measurement error, missing values,
mistakes in the data entry process, or other statistical issues that are common
with that type of data. This yields a dataset $S$. Then, apply the logical
consistency test to $S$. Lastly, repeat this process several times, changing
the number of wrong observations, to create a distribution of summary
statistics from the simulated $S_n$ datasets. This process yields the lower and
upper bounds required for the dataset, assuming that the $S'$ set correctly
approximates the true values of the variable.

While in the section I discuss how rules can be applied to new datasets, they
may also provide interesting insights when used to evaluate existing data. One
possible avenue for research is to compare whether certain conditional
statements produce different outcomes in datasets that measure the same
phenomena, such as level of democracy or changes in political regimes.
Observations that do not conform to the rules should be contrasted and
explained, and if that is case, imputed to credible values. Indeed,
@dewaal2017imputation suggests that imputation methods that both preserve
statistical properties of the variables and conform to rule restrictions are
the best way to fill missing data, even if so far they are difficult to
estimate.

Testing data with conditional expressions provide another benefit for
researchers. Since logical rules can be defined in advance, they may be
included in a preregistration plan. Registering the study design before data
collection or analysis reduces the chances of "data dredging", cases in which
researchers release only the analyses that support their hypotheses
[@klein2018practical]. Preregistration increases the credibility of the
findings and may be even submitted directly for publication in the form of
registered reports [@chambers2013registered]. Journals can evaluate the
research design before scholars know the results, and the manuscript's
acceptance is independent of the final data. As of late 2020, 275 journals
use registered reports as a regular or one-off submission option [@cos2020rr].


## Step 3: Content Validity

Content validity assessment evaluates whether the variables correspond to the
abstract theoretical concept it intends to measure. This is the hardest
validation check and social scientists have written extensively about the
topic. Here I follow @gerring2001social and suggest that researchers check if
their variables meet six criteria: resonance, domain, differentiation,
fecundity, consistency, and causal utility. Resonance means that the variable
reflects an appropriate understanding of the concept. Domain considers when
relevant parties agree with the concept being measured. Differentiation
indicates that the variable should exclude information that do not conform to
the concept of interest. Similarly, fecundity indicates that the variable is
parsimonious and include only the necessary information relevant to each
concept. Consistency is an attribute that signals whether a concept retain its
validity across different settings. Finally, causal utility refers to the idea
that the measure is useful to particular areas of study, which the authors
should enumerate when constructing their data.

## Step 4: Data Generation Validity

This steps corresponds to the relationship between the concepts the researcher
wants to translate into quantifiable information and the data generating
process. More specifically, scholars have to be aware that their data
generating process may be biased or unreliable [@mcmann2016strategies, 12].
With regards to bias, scholars should address whether coders may have
inadvertently introduced their own views into the data collection stage.
If the data generating process is original, it is recommended to verify
intercoder agreement by reporting Cohen's Kappa or Krippendorff's Alpha
[@lombard2002content]. When the data come from
secondary sources and the scholar wants to combine them into an index, it is
good practice to describe how aggregation rules may change the results. One
suggestion is to use Bayesian factor analysis to merge the low-level
indicators, as Bayesian inference adds uncertainty to the resulting index
[@mcmann2016strategies, 15]. 

## Step 5: Convergent Validity

The last step concerns how well the variables compare with well-documented cases.
@mcmann2016strategies suggest that researchers should evaluate their dataset
against other data sources that cover the same topics, including qualitative
studies. We can borrow the idea of validation functions discussed above and apply
the same logic here. However, the statements come from domain knowledge, thus
authors and reviewers should be familiar with the specialised literature in
order to check the consistency of the convergence rules.  

Consider a data set with two variables: `gender` and `salary`. Suppose that a
previous case study in the same location states with high confidence that 20%
of males have earnings lower than $2,000 per month. We can formulate a
validation rule set as follows:

* IF `gender == "male"`
* THEN $\frac{count(\texttt{salary} \hspace{0.1cm} > \hspace{0.1cm} 2,000)}{count(\texttt{salary})} = 0.2$

The example shows how researchers can integrate _a priori_ information to their
estimates. Authors can select the case studies they want to analyse using
different criteria, such as to maximise variation, to evaluate a typical case,
to understand outliers, and so on. I refer the readers to @seawright2008case
for a comprehensive review of case selection methodologies.

Lastly, authors should explain the reasons behind eventual divergences between
their measurements and current theoretical expectations. Validation functions
have the advantage of being transparent to referees and editors, thus making
the assumptions explicit and allowing reviewers to assess the work using
logical statements. That said, it is recommended that authors investigate why
outliers occur and test whether coder characteristics or measurement
construction explain these differences. @mcmann2016strategies [p. 27-36]
provide an empirical case study on how to assess convergent validity.

\newpage

# References
